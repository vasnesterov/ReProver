{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert.infra import ColBERTConfig\n",
    "from colbert.training.lazy_batcher import LazyBatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = '../data/leandojo_benchmark_4/random/colbert_collection.tsv'\n",
    "queries = '../data/leandojo_benchmark_4/random/colbert_queries.json'\n",
    "triples = '../data/leandojo_benchmark_4/random/colbert_triples.json'\n",
    "config = ColBERTConfig(\n",
    "    checkpoint='microsoft/deberta-v3-xsmall',\n",
    "    query_maxlen=512,\n",
    "    doc_maxlen=512,\n",
    "    bsize=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeahrmek/miniconda3/envs/python310/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mar 31, 21:59:28] #> Loading collection...\n"
     ]
    }
   ],
   "source": [
    "batcher = LazyBatcher(\n",
    "    config,\n",
    "    triples,\n",
    "    queries,\n",
    "    collection,\n",
    "    (0 if config.rank == -1 else config.rank),\n",
    "    config.nranks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : Type u_1\n",
      "Y : Type u_2\n",
      "Z : Type u_3\n",
      "inst✝⁴ : TopologicalSpace X\n",
      "inst✝³ : TopologicalSpace Y\n",
      "inst✝² : TopologicalSpace Z\n",
      "X' : Type u_4\n",
      "Y' : Type u_5\n",
      "inst✝¹ : TopologicalSpace X'\n",
      "inst✝ : TopologicalSpace Y'\n",
      "h : X ≃ₜ Y\n",
      "⊢ QuotientMap (↑h ∘ ↑(Homeomorph.symm h))\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . X : Type u_1\n",
      "Y : Type u_2\n",
      "Z : Type u_3\n",
      "inst✝⁴ : TopologicalSpace X\n",
      "inst✝³ : TopologicalSpace Y\n",
      "inst✝² : TopologicalSpace Z\n",
      "X' : Type u_4\n",
      "Y' : Type u_5\n",
      "inst✝¹ : TopologicalSpace X'\n",
      "inst✝ : TopologicalSpace Y'\n",
      "h : X ≃ₜ Y\n",
      "⊢ QuotientMap (↑h ∘ ↑(Homeomorph.symm h)), \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([512]), tensor([     1, 128001,   1477,    877,   5004,   3636,    616,    435,   2316,\n",
      "           877,   5004,   3636,    616,    445,   2548,    877,   5004,   3636,\n",
      "           616,    508,    267,   1606, 126384,    554,    877,   2482,  13669,\n",
      "         21315,   1477,    267,   1606, 126384,    508,    877,   2482,  13669,\n",
      "         21315,   2316,    267,   1606, 126384,    445,    877,   2482,  13669,\n",
      "         21315,   2548,   1477,    280,    877,   5004,   3636,    616,    554,\n",
      "          2316,    280,    877,   5004,   3636,    616,    524,    267,   1606,\n",
      "        126384,    435,    877,   2482,  13669,  21315,   1477,    280,    267,\n",
      "          1606, 126384,    877,   2482,  13669,  21315,   2316,    280,   3807,\n",
      "           877,   1477,    507, 125048,    297,   2316,    507, 127011, 114467,\n",
      "         22709,    287, 123360,   1537,    507, 122588,  13937,    555,  13113,\n",
      "         64988,    260,  41282,    358,   3807,    285,    285,      2, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000,\n",
      "        128000, 128000, 128000, 128000, 128000, 128000, 128000, 128000])\n",
      "#> Output Mask: torch.Size([512]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in batcher:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS][Q] X : Type u_1 Y : Type u_2 Z : Type u_3 inst✝4 : TopologicalSpace X inst✝3 : TopologicalSpace Y inst✝2 : TopologicalSpace Z X' : Type u_4 Y' : Type u_5 inst✝1 : TopologicalSpace X' inst✝ : TopologicalSpace Y' h : X ≃t Y ⊢ QuotientMap (↑h ∘ ↑(Homeomorph.symm h))[SEP][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK]\n"
     ]
    }
   ],
   "source": [
    "print(batcher.query_tokenizer.tok.decode(batch[0][0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

seed_everything: 3407  # https://arxiv.org/abs/2109.08203
trainer:
  accelerator: gpu
  devices: 1
  precision: bf16-mixed
  strategy:
    class_path: pytorch_lightning.strategies.DeepSpeedStrategy
    init_args:
      stage: 2
      offload_optimizer: false
      cpu_checkpointing: false
  gradient_clip_val: 1.0
  max_steps: 800000
  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        verbose: true
        save_top_k: 1
        save_last: true
        monitor: val/Recall@10
        mode: max
  check_val_every_n_epoch: 2

  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      project: lean-colbert

model:
  num_retrieved: 20
  lr: 1e-5
  warmup_steps: 20000
  config:
    checkpoint: microsoft/deberta-v3-xsmall
    collection: ./data/msmarco/collection.tsv
    index_name: msmarco
    experiment: msmarco
    root: ./experiments/

    nbits: 1
    kmeans_niters: 20
    resume: false
    similarity: cosine
    relu: false
    nway: 16
    use_ib_negatives: true
    drop_duplciate_passages: false
    reranker: false
    distillation_alpha: 1.0
    ignore_scores: false
    query_maxlen: 128
    doc_maxlen: 180
    attend_to_mask_tokens: false
    interaction: colbert
    dim: 128
    mask_punctuation: true
    overwrite: false

data:
  data_path: data/msmarco
  batch_size: 24
  eval_batch_size: 128 # 128
  num_workers: 4
